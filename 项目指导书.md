# 基于BBDM的动漫线稿参考图引导着色系统 - 项目整体概述

---

## 1. 项目目标与背景

### 1.1 项目目标
基于布朗桥扩散模型（BBDM）实现参考图引导的动漫线稿自动上色，生成高保真、风格一致、边缘锐利的彩色图像。

### 1.2 核心问题
本项目致力于解决动漫线稿上色任务中的三大核心痛点：
1. **语义层级控制混乱**：传统方法难以区分"涂什么颜色"与"整体色调"，导致颜色溢出或风格不统一
2. **参考图与线稿空间不匹配**：姿态/视角差异导致颜色错位或伪影
3. **生成图像边缘模糊**：扩散模型难以保持线稿原本锐利的边缘

### 1.3 技术路线概览
采用 **"潜空间生成 + 统一条件编码 + 像素级修复"** 的三层架构：

**核心模块**：
1. **VQGAN**：图像压缩/重建（预训练冻结）
2. **UCE-Lite v2**：统一条件编码器（双塔 + Cross-Attention 对齐）
3. **Latent BBDM**：潜空间布朗桥扩散生成主干
4. **自适应边缘细化模块**：后处理修复(可选)

**核心创新**：
1. **UCE-Lite v2 统一条件编码**：将线稿结构提取、参考图特征提取、特征对齐整合为单一轻量模块
2. **稳定 Cross-Attention 对齐**：Cosine + 温度调度 + Dropout，实现鲁棒的语义对应
3. **分层条件注入**：多尺度特征金字塔分层注入 U-Net
4. **四阶段渐进训练**：从全局风格到局部对齐，课程学习式训练

---

## 2. 系统架构与数据流

### 2.1 整体架构图
```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                 输入层                                       │
│         线稿 [B,1,H,W]                    参考彩图 [B,3,H,W]                 │
└──────────────┬─────────────────────────────────┬────────────────────────────┘
               │                                 │
               ▼                                 ▼
┌──────────────────────────┐      ┌────────────────────────────────────────────┐
│   VQGAN Encoder (冻结)   │      │            UCE-Lite v2                     │
│   lineart → latent       │      │  ┌─────────────┐    ┌─────────────┐        │
│                          │      │  │ Lineart     │    │ Reference   │        │
│                          │      │  │ Tower       │    │ Tower       │        │
│                          │      │  │ [B,64,H/4]  │    │ [B,64,H/4]  │        │
│                          │      │  └──────┬──────┘    └──────┬──────┘        │
│                          │      │         │    Cross-Attn    │               │
│                          │      │         └────────┬─────────┘               │
│                          │      │                  ▼                         │
│                          │      │         Aligned Features                   │
│                          │      │                  ▼                         │
│                          │      │      ┌──────────┴──────────┐               │
│                          │      │      ▼          ▼          ▼               │
│                          │      │  style_vec    p2         p3                │
│                          │      │  [B,64]    [B,C,H/16] [B,C,H/32]           │
└──────────────┬───────────┘      └──────┬──────────┬──────────┬───────────────┘
               │                         │          │          │
               ▼                         ▼          ▼          ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                          BBDM U-Net 主干                                     │
│  ┌─────────┐      ┌─────────┐      ┌─────────┐      ┌─────────┐             │
│  │ down_0  │ ───→ │ down_1  │ ───→ │ middle  │ ───→ │  up_0   │ ───→ output │
│  │ AdaIN   │      │         │      │CrossAttn│      │  FiLM   │             │
│  │style_vec│      │         │      │   p3    │      │   p2    │             │
│  └─────────┘      └─────────┘      └─────────┘      └─────────┘             │
│       ↑                                                                      │
│  x_cond (线稿 latent，布朗桥起点)                                            │
└──────────────────────────────────────┬───────────────────────────────────────┘
                                       │
                                       ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                         VQGAN Decoder (冻结)                                 │
│                      latent → 粗糙彩图 [B,3,H,W]                             │
└──────────────────────────────────────┬───────────────────────────────────────┘
                                       │
                                       ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                        自适应边缘细化模块                                     │
│                   粗糙彩图 + 原始线稿 → 最终输出                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 训练流程

```python
# 1. 加载配对数据
(target_color, lineart, reference) = batch
# target_color: 目标彩图 [B,3,H,W]
# lineart: 输入线稿 [B,1,H,W]
# reference: 参考彩图 [B,3,H,W]（可能有形变）

# 2. UCE-Lite v2 提取条件特征
style_vec, p2, p3 = UCE(lineart, reference)
# style_vec: [B,64] 全局风格
# p2: [B,128,H/16,W/16] 中尺度对齐特征
# p3: [B,192,H/32,W/32] 粗尺度对齐特征

# 3. VQGAN 编码到潜空间（冻结）
y_0 = VQGAN.encode(target_color)      # [B,4,H/8,W/8] 目标潜变量
x_cond = VQGAN.encode(lineart_rgb)    # [B,4,H/8,W/8] 布朗桥起点

# 4. 布朗桥前向加噪
t = random_timestep()
m_t = get_bridge_coef(t)
y_t = (1 - m_t) * y_0 + m_t * x_cond + sigma_t * noise

# 5. U-Net 预测（分层注入条件特征）
cond_dict = {'style_vec': style_vec, 'p2': p2, 'p3': p3}
objective_pred = UNet(y_t, t, cond_dict)

# 6. 计算损失
loss = BBDMLoss(objective_pred, target) + PerceptualLoss + StyleLoss
```

### 2.3 推理流程

```python
# 1. UCE 提取条件特征
style_vec, p2, p3 = UCE(lineart, reference)

# 2. 初始化：从线稿 latent 出发
x_cond = VQGAN.encode(lineart_rgb)
y_T = x_cond + noise

# 3. 布朗桥反向采样
for t in reversed(timesteps):
    cond_dict = {'style_vec': style_vec, 'p2': p2, 'p3': p3}
    objective_pred = UNet(y_t, t, cond_dict)
    y_t = bridge_step(y_t, objective_pred, t)

# 4. 解码 + 边缘细化
coarse_color = VQGAN.decode(y_0_pred)
final_output = Refinement(coarse_color, lineart)
```

### 2.4 数据流说明

| 阶段 | 输入 | 输出 | 说明 |
|:---|:---|:---|:---|
| **UCE 编码** | 线稿 + 参考图 | style_vec, p2, p3 | 双塔提取 + Cross-Attn 对齐 |
| **VQGAN 编码** | 线稿/目标彩图 | x_cond / y_0 | 压缩到潜空间（冻结） |
| **BBDM 扩散** | y_t + 条件特征 | y_0_pred | 布朗桥去噪生成 |
| **VQGAN 解码** | y_0_pred | 粗糙彩图 | 还原到像素空间（冻结） |
| **边缘细化** | 粗糙彩图 + 线稿 | 最终输出 | 修复边缘模糊 |

---

## 3. 核心模块详解

### 3.1 UCE-Lite v2（统一条件编码器）

UCE-Lite v2 将原有的多尺度结构编码器、轻量级特征提取器、特征融合模块、流场对齐四个组件统一为一个模块，大幅简化架构同时保留核心能力。

#### 3.1.1 设计动机

UCE-Lite v2 采用:  双塔架构，参数共享, Cross-Attention 软对齐, 注意力隐式对齐, 单一模块，端到端训练 |

#### 3.1.2 架构设计

```
输入:
  - lineart: [B, 1, H, W]
  - reference: [B, 3, H, W]

                       ┌─────────────────────┐
                       │    Lineart Tower    │
       lineart ───────→│ Conv 7×7 → Res×2    │───→ L_feat [B, 64, H/4, W/4]
                       │ (输出 64 通道)       │         │
                       └─────────────────────┘         │
                                                       │
                       ┌─────────────────────┐         │
                       │   Reference Tower   │         │
      reference ──────→│ Conv 7×7 → Res×2    │───→ R_feat [B, 64, H/4, W/4]
                       │ (输出 64 通道)       │         │
                       └─────────────────────┘         │
                                                       ▼
                       ┌──────────────────────────────────────────┐
                       │         Stable Cross-Attention          │
                       │  Q = L_feat,  K = V = R_feat            │
                       │  ───────────────────────────────────     │
                       │  sim = cosine(Q, K)                     │
                       │  logits = sim / τ(t)   # τ: 1.5 → 0.5   │
                       │  logits = clip(logits, -10, +10)        │
                       │  attn = softmax(logits + dropout)       │
                       │  aligned = attn @ V + residual          │
                       └──────────────────────────────────────────┘
                                         │
                                         ▼
                       ┌──────────────────────────────────────────┐
                       │            Pyramid Head                 │
                       │  ┌─────────────────────────────────────┐ │
                       │  │ Global Pool → FC → style_vec [B,64]│ │
                       │  ├─────────────────────────────────────┤ │
                       │  │ Conv 1×1 + Down → p2 [B,128,H/16]  │ │
                       │  ├─────────────────────────────────────┤ │
                       │  │ Conv 1×1 + Down → p3 [B,192,H/32]  │ │
                       │  └─────────────────────────────────────┘ │
                       └──────────────────────────────────────────┘

输出:
  - style_vec: [B, 64]           # 全局风格，用于 AdaIN
  - p2: [B, 128, H/16, W/16]     # 中尺度对齐特征，用于 FiLM
  - p3: [B, 192, H/32, W/32]     # 粗尺度对齐特征，用于 CrossAttn
```

#### 3.1.3 Stable Cross-Attention 设计

| 组件 | 作用 | 配置 |
|:---|:---|:---|
| **Cosine Similarity** | 归一化相似度，避免尺度爆炸 | `sim = Q·K / (‖Q‖·‖K‖)` |
| **Temperature Schedule** | 早期平滑，后期锐化 | τ: 1.5 → 0.5 线性退火 |
| **Dropout** | 防止注意力塌缩到单点 | rate = 0.1 |
| **Logit Clipping** | 截断极端值，防止 NaN | ±10 |
| **Residual Connection** | 保留原始特征 | 0.8·aligned + 0.2·original |

#### 3.1.4 代码实现框架

```python
class UCELiteV2(nn.Module):
    def __init__(self, in_ch_lineart=1, in_ch_ref=3, base_ch=64):
        super().__init__()
        # 双塔 Stem（不共享权重）
        self.lineart_stem = nn.Sequential(
            nn.Conv2d(in_ch_lineart, base_ch, 7, 2, 3),
            nn.GroupNorm(8, base_ch), nn.SiLU(),
            ResBlock(base_ch, base_ch),
            nn.Conv2d(base_ch, base_ch, 3, 2, 1),  # H/4
        )
        self.ref_stem = nn.Sequential(
            nn.Conv2d(in_ch_ref, base_ch, 7, 2, 3),
            nn.GroupNorm(8, base_ch), nn.SiLU(),
            ResBlock(base_ch, base_ch),
            nn.Conv2d(base_ch, base_ch, 3, 2, 1),  # H/4
        )
        # Stable Cross-Attention
        self.cross_attn = StableCrossAttention(base_ch, num_heads=4)
        # 输出头
        self.style_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1), nn.Flatten(),
            nn.Linear(base_ch, base_ch)
        )
        self.p2_head = nn.Sequential(
            nn.Conv2d(base_ch, 128, 1), nn.AvgPool2d(4)  # H/4 → H/16
        )
        self.p3_head = nn.Sequential(
            nn.Conv2d(base_ch, 192, 1), nn.AvgPool2d(8)  # H/4 → H/32
        )
    
    def forward(self, lineart, reference, temperature=1.0):
        L_feat = self.lineart_stem(lineart)    # [B, 64, H/4, W/4]
        R_feat = self.ref_stem(reference)       # [B, 64, H/4, W/4]
        aligned = self.cross_attn(L_feat, R_feat, temperature)
        
        style_vec = self.style_head(aligned)   # [B, 64]
        p2 = self.p2_head(aligned)              # [B, 128, H/16, W/16]
        p3 = self.p3_head(aligned)              # [B, 192, H/32, W/32]
        return style_vec, p2, p3
```

### 3.2 VQGAN 潜空间编解码（冻结）

VQGAN 使用预训练权重，**在整个训练过程中完全冻结**，仅负责像素空间与潜空间的转换。

| 属性 | 值 |
|:---|:---|
| 压缩比 | f8 (512 → 64 空间尺寸) |
| 潜变量通道 | 4 |
| 向量量化 | 是，codebook 大小 16384 |
| 训练状态 | 完全冻结 |

**职责边界**：VQGAN 仅负责**图像重建**，而 UCE-Lite v2 负责**语义特征提取与对齐**，二者各司其职，不存在冗余。

### 3.3 BBDM U-Net 条件注入策略

UCE-Lite v2 的三路输出分别注入 U-Net 的不同层级：

| 注入位置 | 注入方式 | 注入特征 | 作用 |
|:---|:---|:---|:---|
| `down_0` | **AdaIN** | style_vec [B,64] | 全局色调、风格 |
| `middle` | **CrossAttn** | p3 [B,192,H/32] | 粗粒度语义对应 |
| `up_0` | **FiLM** | p2 [B,128,H/16] | 局部颜色细节 |

```python
# 注入代码示例
class InjectionManager:
    def inject(self, block_name, features, cond_dict, t):
        if block_name == 'down_0':
            # AdaIN: 调制均值和方差
            gamma, beta = self.style_to_affine(cond_dict['style_vec'])
            features = gamma * normalize(features) + beta
        elif block_name == 'middle':
            # Cross-Attention: 特征与 p3 做注意力
            features = self.cross_attn(features, cond_dict['p3'])
        elif block_name == 'up_0':
            # FiLM: 逐像素调制
            gamma, beta = self.spatial_affine(cond_dict['p2'])
            features = gamma * features + beta
        return features
```

### 3.4 自适应边缘细化模块(可选)

VQGAN 解码会导致边缘模糊，该模块负责最终修复：

```python
class AdaptiveRefinement(nn.Module):
    def __init__(self):
        self.edge_detector = SobelFilter()
        self.fusion = nn.Sequential(
            nn.Conv2d(4, 32, 3, 1, 1),  # 3 (color) + 1 (lineart)
            nn.SiLU(),
            ResBlock(32, 32),
            nn.Conv2d(32, 3, 3, 1, 1)
        )
    
    def forward(self, coarse_color, lineart):
        edge_mask = self.edge_detector(lineart)
        x = torch.cat([coarse_color, lineart], dim=1)
        residual = self.fusion(x)
        # 仅在边缘区域应用修正
        return coarse_color + edge_mask * residual
```

---

## 4. 训练策略

### 4.1 四阶段递进训练

> **规划说明**：以下 Epoch 设置基于典型动漫数据集规模（约 30,000 - 50,000 张图像）。若数据集较小（<10k），建议按比例增加 Epoch 数，以确保总迭代次数达到 300k+ 标准。

#### 阶段 0：VQGAN 预训练（可跳过）
- **目标**：获得高质量的潜空间编解码器
- **策略**：使用现成预训练权重（如 Stable Diffusion VAE）或在动漫数据集上微调
- **状态**：训练完成后**永久冻结**，不参与后续训练

#### 阶段 1：BBDM + UCE-Global（约 50 epochs）
- **目标**：建立稳定的潜空间扩散基础，仅依赖全局风格引导
- **UCE 配置**：
  - 仅训练 `style_vec` 分支，冻结 p2/p3 输出头
  - Cross-Attention 使用高温度（τ=2.0），输出平滑
- **注入策略**：
  - 仅启用 `down_0` AdaIN 注入（全局风格）
  - `middle` 和 `up_0` 注入关闭
- **数据策略**：增强自参考（同一彩图提取线稿 + 原图加噪作为参考）
- **模块配置**：
  | 模块 | 状态 |
  |:---|:---|
  | VQGAN | 冻结 |
  | UCE-Lite v2 (style_vec) | 训练 |
  | UCE-Lite v2 (p2, p3) | 冻结/零输出 |
  | BBDM U-Net | 训练 |
  | Refinement | 冻结 |
- **损失权重**：`{bbdm: 1.0, perceptual: 0.05}`

#### 阶段 2：UCE 完整 + BBDM 微调（约 40 epochs）
- **目标**：启用完整的 UCE 对齐能力，学习局部颜色映射
- **UCE 配置**：
  - 解冻 p2/p3 输出头，完整前向传播
  - Cross-Attention 温度线性退火（τ: 1.5 → 0.5）
- **注入策略**：
  - 启用全部三路注入：AdaIN + CrossAttn + FiLM
  - 注入强度从 0.3 线性升到 1.0
- **数据策略**：真实配对数据 + 中等 TPS 变形增强
- **模块配置**：
  | 模块 | 状态 |
  |:---|:---|
  | VQGAN | 冻结 |
  | UCE-Lite v2 | 完整训练 |
  | BBDM U-Net | 微调 (LR × 0.5) |
  | Refinement | 冻结 |
- **损失权重**：`{bbdm: 1.0, perceptual: 0.15, style: 0.1}`

#### 阶段 3：全链路微调 + Refinement（约 30 epochs）
- **目标**：端到端优化，修复边缘模糊
- **UCE 配置**：温度固定为 τ=0.5（锐化注意力）
- **注入策略**：注入强度稳定，不再调节
- **模块配置**：
  | 模块 | 状态 |
  |:---|:---|
  | VQGAN | 冻结 |
  | UCE-Lite v2 | 微调 (LR × 0.3) |
  | BBDM U-Net | 微调 (LR × 0.1) |
  | Refinement | 训练 |
- **损失权重**：`{bbdm: 0.8, perceptual: 0.2, style: 0.15, edge: 0.3}`

### 4.2 损失函数体系

| 损失类型 | 计算方式 | 作用 |
|:---|:---|:---|
| BBDM 主损失 | 布朗桥噪声预测 MSE | 核心生成目标 |
| LPIPS 感知损失 | VGG 特征距离 | 保证感知质量 |
| 风格一致性损失 | Gram 矩阵距离 | 匹配参考图风格 |
| 边缘细化损失 | 边缘 L1 + 梯度 L2 | 强化边缘锐度 |

### 4.3 数据增强策略

- **课程学习式 TPS 变形**（`datasets/tps.py`）：
  - 阶段 1：轻微变形（distortion_scale=0.05），模拟自参考
  - 阶段 2：中等变形（0.1-0.2），逐步引入对齐挑战
  - 阶段 3：较强变形（0.3+），增强鲁棒性
- **颜色抖动**：亮度/饱和度/色调随机扰动，增强风格泛化

### 4.4 消融实验设计

为验证 UCE-Lite v2 各组件的贡献，设计以下消融实验：

| 配置 | 描述 | 预期结果 |
|:---|:---|:---|
| **A: Baseline** | 无条件 BBDM（仅 VQGAN latent） | 生成随机颜色，无参考指导 |
| **B: +Global** | +UCE style_vec + AdaIN | 整体色调正确，但局部颜色随机 |
| **C: +Align** | +UCE p2/p3 + CrossAttn/FiLM | 局部颜色对应正确 |
| **D: +Refine** | +Refinement 模块 | 边缘锐利，颜色不溢出 |

**评估指标**：
- FID↓：图像质量
- LPIPS↓：感知相似度
- Color Consistency：参考图颜色匹配度
- Edge Precision：边缘区域颜色准确率

---

## 5. 项目结构

```
BBDM/
├── configs/
│   └── AnimeLineartColor.yaml          # 主配置文件
├── datasets/
│   ├── custom.py                       # 数据集定义（lineart_reference）
│   └── tps.py                          # TPS 变形增强
├── model/
│   ├── encoder/
│   │   └── uce_lite_v2.py              # 统一条件编码器（核心）
│   ├── BrownianBridge/
│   │   ├── BrownianBridgeModel.py            # 基础 BBDM
│   │   ├── LatentBrownianBridgeModel.py      # 潜空间 BBDM
│   │   ├── injection/
│   │   │   ├── base_injection.py             # 注入基类（AdaIN/CrossAttn/FiLM）
│   │   │   └── injection_manager.py          # 注入管理器
│   │   └── base/modules/
│   │       └── diffusionmodules/openaimodel.py  # U-Net 主干
│   ├── refinement/
│   │   └── adaptive_edge_refinement.py       # 边缘细化模块
│   └── losses/
│       └── integrated_loss_manager.py        # 统一损失管理器
├── runners/
│   ├── LineartColorRunner.py                 # 专用训练 Runner
│   └── components/
│       ├── stage_manager.py                  # 训练阶段管理
│       └── metrics_manager.py                # 评估指标管理
├── evaluation/
│   ├── FID.py, LPIPS.py, diversity.py        # 标准评测
│   └── [待添加] color_consistency.py         # 色彩一致性指标
└── main.py                                   # 入口
```

---

## 6. 配置与运行

### 6.1 关键配置项（AnimeLineartColor.yaml）

```yaml
model:
  model_type: "LBBDM"  # Latent BBDM
  
  # UCE-Lite v2 配置
  uce:
    base_channels: 64
    cross_attention:
      num_heads: 4
      temperature_init: 1.5
      temperature_final: 0.5
      dropout: 0.1
      logit_clip: 10.0
    output_channels:
      style_vec: 64
      p2: 128
      p3: 192
  
  # 注入配置
  injection:
    stages:
      down_0: {type: adain, feature: style_vec}
      middle: {type: cross_attention, feature: p3}
      up_0:   {type: film, feature: p2}

training:
  stages:
    stage_1: {epochs: 50, uce_mode: global_only}
    stage_2: {epochs: 40, uce_mode: full}
    stage_3: {epochs: 30, uce_mode: full, refinement: true}
  
loss_weights:
  stage_1: {bbdm: 1.0, perceptual: 0.05}
  stage_2: {bbdm: 1.0, perceptual: 0.15, style: 0.1}
  stage_3: {bbdm: 0.8, perceptual: 0.2, style: 0.15, edge: 0.3}

data:
  # 阶段一使用增强自参考
  stage1_self_reference: true
  tps_curriculum: [0.05, 0.15, 0.3]  # 各阶段 TPS 强度
```

### 6.2 运行命令
```bash
# 训练
python main.py --config configs/AnimeLineartColor.yaml --mode train

# 推理
python main.py --config configs/AnimeLineartColor.yaml --mode sample --ckpt_path <checkpoint>
```

---

## 7. 评测指标

| 指标 | 说明 | 目标 |
|:---|:---|:---|
| FID | 生成质量（Inception 特征分布距离） | ↓ 越低越好 |
| LPIPS | 感知相似度 | ↓ 越低越好 |
| 色彩一致性 | 与参考图 LAB 直方图 EMD 距离 | ↓ 越低越好 |
| 边缘保真度 | 生成边缘与线稿的 IoU/F1 | ↑ 越高越好 |
| 用户偏好 | 主观评测 | ↑ 越高越好 |

---

## 8. 核心创新点总结

1. **UCE-Lite v2 统一条件编码器**
   - 将原有 4 个分散模块（结构编码器 + 特征提取器 + 融合模块 + 流场对齐）整合为单一模块
   - 双塔架构 + Stable Cross-Attention 实现隐式对齐，无需显式光流估计

2. **Stable Cross-Attention 机制**
   - Cosine 相似度 + 温度退火（τ: 1.5 → 0.5）+ Dropout + Logit Clipping
   - 解决传统注意力的数值不稳定和注意力塌缩问题

3. **分层条件注入策略**
   - AdaIN（全局风格）→ CrossAttn（粗粒度语义）→ FiLM（局部细节）
   - 不同粒度的特征注入到 U-Net 的相应层级

4. **四阶段递进训练**
   - 阶段 0: VQGAN 预训练
   - 阶段 1: BBDM + UCE-Global（建立扩散基础）
   - 阶段 2: UCE 完整 + BBDM 微调（学习局部对齐）
   - 阶段 3: 全链路微调 + Refinement（修复边缘）

---

## 9. 风险与应对

| 风险 | 应对策略 |
|:---|:---|
| 训练不稳定 | 严格分阶段训练，梯度裁剪，学习率热启动 |
| Cross-Attention 数值问题 | Cosine 相似度 + Logit Clipping + 温度退火 |
| 极端姿态对齐失败 | 隐式注意力对齐优于显式光流，允许多对一映射 |
| 推理速度慢 | 特征缓存，混合精度，模型蒸馏（后期优化） |
| 边缘模糊 | Refinement 模块 + 边缘损失加权 |

---

## 10. 总结

本项目采用 **"潜空间生成 + 统一条件编码 + 像素级修复"** 的三层架构：

| 层级 | 模块 | 职责 |
|:---|:---|:---|
| **编码层** | VQGAN (冻结) | 像素 ↔ 潜空间转换，图像压缩与重建 |
| **条件层** | UCE-Lite v2 | 提取 + 对齐，输出 style_vec/p2/p3 |
| **生成层** | BBDM U-Net | 潜空间布朗桥扩散，接收条件注入 |
| **修复层** | Refinement | 像素空间边缘细化 |

**核心设计理念**：
- **职责分离**：VQGAN 负责重建，UCE 负责语义控制，二者不冗余
- **软对齐优于硬对齐**：Cross-Attention 隐式对齐比显式光流更鲁棒
- **递进式训练**：先学全局色调，再学局部映射，最后修复边缘

通过 UCE-Lite v2 的统一设计和四阶段训练策略，提供了一套简洁高效的动漫线稿自动着色解决方案。
